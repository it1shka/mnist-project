{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "tf = ToTensor()\n",
    "train_dataset = MNIST(root='./data', train=True, transform=tf, download=True)\n",
    "test_dataset = MNIST(root='./data', train=False, transform=tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3136"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64 * 7 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class DigitRecognitionCNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.extraction_base = nn.Sequential(\n",
    "      nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "      nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    )\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.classification_head = nn.Sequential(\n",
    "      nn.Linear(64 * 7 * 7, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.1),\n",
    "      nn.Linear(128, 10),\n",
    "    )\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.extraction_base(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.classification_head(x)\n",
    "    return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = (\n",
    "  'cuda' if torch.cuda.is_available()\n",
    "  else 'mps' if torch.backends.mps.is_available()\n",
    "  else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DigitRecognitionCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1039, 0.1007, 0.0989, 0.0953, 0.0932, 0.0916, 0.1082, 0.1066, 0.0915,\n",
       "         0.1101],\n",
       "        [0.1047, 0.1021, 0.1011, 0.0933, 0.0917, 0.0917, 0.1055, 0.1080, 0.0910,\n",
       "         0.1111],\n",
       "        [0.1047, 0.1048, 0.1011, 0.0916, 0.0882, 0.0919, 0.1059, 0.1076, 0.0923,\n",
       "         0.1118],\n",
       "        [0.1047, 0.1021, 0.0998, 0.0923, 0.0907, 0.0912, 0.1048, 0.1124, 0.0932,\n",
       "         0.1088],\n",
       "        [0.1046, 0.1035, 0.0997, 0.0939, 0.0899, 0.0898, 0.1061, 0.1083, 0.0926,\n",
       "         0.1117],\n",
       "        [0.1027, 0.1021, 0.1013, 0.0939, 0.0915, 0.0910, 0.1064, 0.1089, 0.0920,\n",
       "         0.1103],\n",
       "        [0.1044, 0.1031, 0.1005, 0.0938, 0.0909, 0.0912, 0.1057, 0.1081, 0.0928,\n",
       "         0.1096],\n",
       "        [0.1027, 0.1033, 0.1022, 0.0941, 0.0896, 0.0919, 0.1044, 0.1090, 0.0930,\n",
       "         0.1099],\n",
       "        [0.1039, 0.1032, 0.0995, 0.0927, 0.0895, 0.0912, 0.1052, 0.1119, 0.0923,\n",
       "         0.1106],\n",
       "        [0.1045, 0.1032, 0.0996, 0.0926, 0.0902, 0.0922, 0.1032, 0.1112, 0.0927,\n",
       "         0.1107],\n",
       "        [0.1045, 0.1039, 0.1007, 0.0936, 0.0903, 0.0916, 0.1040, 0.1089, 0.0912,\n",
       "         0.1113],\n",
       "        [0.1034, 0.1017, 0.1031, 0.0941, 0.0913, 0.0904, 0.1061, 0.1087, 0.0928,\n",
       "         0.1084],\n",
       "        [0.1033, 0.1032, 0.0999, 0.0953, 0.0908, 0.0917, 0.1072, 0.1074, 0.0914,\n",
       "         0.1098],\n",
       "        [0.1027, 0.1019, 0.1022, 0.0938, 0.0897, 0.0909, 0.1066, 0.1089, 0.0922,\n",
       "         0.1110],\n",
       "        [0.1026, 0.1018, 0.1029, 0.0928, 0.0879, 0.0918, 0.1049, 0.1141, 0.0921,\n",
       "         0.1091],\n",
       "        [0.1049, 0.1027, 0.1028, 0.0938, 0.0900, 0.0921, 0.1050, 0.1054, 0.0925,\n",
       "         0.1106],\n",
       "        [0.1031, 0.1024, 0.1002, 0.0941, 0.0905, 0.0918, 0.1061, 0.1093, 0.0920,\n",
       "         0.1105],\n",
       "        [0.1026, 0.1024, 0.1022, 0.0941, 0.0901, 0.0934, 0.1064, 0.1073, 0.0922,\n",
       "         0.1094],\n",
       "        [0.1039, 0.1017, 0.1016, 0.0931, 0.0906, 0.0922, 0.1058, 0.1100, 0.0914,\n",
       "         0.1095],\n",
       "        [0.1042, 0.1011, 0.1007, 0.0933, 0.0910, 0.0924, 0.1059, 0.1111, 0.0920,\n",
       "         0.1083],\n",
       "        [0.1038, 0.1014, 0.1010, 0.0937, 0.0912, 0.0908, 0.1081, 0.1095, 0.0927,\n",
       "         0.1080],\n",
       "        [0.1030, 0.1022, 0.0995, 0.0955, 0.0928, 0.0909, 0.1065, 0.1091, 0.0908,\n",
       "         0.1097],\n",
       "        [0.1037, 0.1013, 0.1032, 0.0934, 0.0895, 0.0915, 0.1092, 0.1064, 0.0933,\n",
       "         0.1086],\n",
       "        [0.1054, 0.1003, 0.1025, 0.0923, 0.0905, 0.0899, 0.1043, 0.1130, 0.0945,\n",
       "         0.1073],\n",
       "        [0.1018, 0.1026, 0.1028, 0.0944, 0.0903, 0.0908, 0.1098, 0.1074, 0.0920,\n",
       "         0.1081],\n",
       "        [0.1035, 0.1026, 0.1013, 0.0942, 0.0900, 0.0927, 0.1055, 0.1077, 0.0918,\n",
       "         0.1107],\n",
       "        [0.1034, 0.1026, 0.1014, 0.0927, 0.0906, 0.0912, 0.1066, 0.1105, 0.0927,\n",
       "         0.1083],\n",
       "        [0.1043, 0.1034, 0.1022, 0.0939, 0.0900, 0.0907, 0.1074, 0.1057, 0.0924,\n",
       "         0.1101],\n",
       "        [0.1052, 0.1007, 0.1004, 0.0933, 0.0912, 0.0907, 0.1072, 0.1102, 0.0934,\n",
       "         0.1078],\n",
       "        [0.1059, 0.1031, 0.1020, 0.0937, 0.0899, 0.0912, 0.1045, 0.1081, 0.0914,\n",
       "         0.1103],\n",
       "        [0.1051, 0.1014, 0.0975, 0.0941, 0.0900, 0.0901, 0.1069, 0.1121, 0.0917,\n",
       "         0.1111],\n",
       "        [0.1055, 0.1043, 0.1005, 0.0951, 0.0903, 0.0918, 0.1050, 0.1068, 0.0910,\n",
       "         0.1097]], device='mps:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get first batch\n",
    "images, _ = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "prediction = model(images)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: average loss = 1.5697455500284831\n",
      "Epoch 1: average loss = 1.4849194522857665\n",
      "Epoch 2: average loss = 1.479354282951355\n",
      "Epoch 3: average loss = 1.4768625860850015\n",
      "Epoch 4: average loss = 1.4752865712483725\n",
      "Epoch 5: average loss = 1.4741421333312987\n",
      "Epoch 6: average loss = 1.4727361528396608\n",
      "Epoch 7: average loss = 1.4721535385131836\n",
      "Epoch 8: average loss = 1.4713026896794636\n",
      "Epoch 9: average loss = 1.4709596195856731\n",
      "Epoch 10: average loss = 1.4699616762161254\n",
      "Epoch 11: average loss = 1.47053470287323\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m   loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     10\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 11\u001b[0m   total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     12\u001b[0m   total_tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     13\u001b[0m average_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m total_tries\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "total_loss, total_tries = 0, 0\n",
    "for current_epoch in range(epochs):\n",
    "  for images, labels in train_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(images)\n",
    "    loss = loss_function(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "    total_tries += 1\n",
    "  average_loss = total_loss / total_tries\n",
    "  print(f'Epoch {current_epoch}: average loss = {average_loss}')\n",
    "  total_loss, total_tries = 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  98.85%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on a test set\n",
    "correct, total = 0, 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for images, labels in test_loader:\n",
    "    # print(images.shape)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    predictions = model(images)\n",
    "    # print(predictions.shape)\n",
    "    _, predicted = torch.max(predictions.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy: .2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "postfix = str(round(accuracy * 1e4))\n",
    "path = f'./models/model_{postfix}.pth'\n",
    "torch.save(model, path)\n",
    "state = model.state_dict()\n",
    "torch.save(state, f'{path}.state')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
